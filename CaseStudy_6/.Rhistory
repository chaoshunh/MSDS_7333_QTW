100 * (totchar - totplain)/totchar
},
subSpamWords =
function(msg)
{
if("Subject" %in% names(msg$header))
length(grep(paste(SpamCheckWords, collapse = "|"),
tolower(msg$header["Subject"]))) > 0
else
NA
}
,
subBlanks =
function(msg)
{
if("Subject" %in% names(msg$header)) {
x = msg$header["Subject"]
# should we count blank subject line as 0 or 1 or NA?
if (nchar(x) == 1) return(0)
else 100 *(1 - (nchar(gsub("[[:blank:]]", "", x))/nchar(x)))
} else NA
}
,
noHost =
function(msg)
{
# Or use partial matching.
idx = pmatch("Message-", names(msg$header))
if(is.na(idx)) return(NA)
tmp = msg$header[idx]
return(length(grep(".*@[^[:space:]]+", tmp)) ==  0)
}
,
numEnd =
function(msg)
{
# If we just do a grep("[0-9]@",  )
# we get matches on messages that have a From something like
# " \"marty66@aol.com\" <synjan@ecis.com>"
# and the marty66 is the "user's name" not the login
# So we can be more precise if we want.
x = names(msg$header)
if ( !( "From" %in% x) ) return(NA)
login = gsub("^.*<", "", msg$header["From"])
if ( is.null(login) )
login = gsub("^.*<", "", msg$header["X-From"])
if ( is.null(login) ) return(NA)
login = strsplit(login, "@")[[1]][1]
length(grep("[0-9]+$", login)) > 0
},
isYelling =
function(msg)
{
if ( "Subject" %in% names(msg$header) ) {
el = gsub("[^[:alpha:]]", "", msg$header["Subject"])
if (nchar(el) > 0) nchar(gsub("[A-Z]", "", el)) < 1
else FALSE
}
else
NA
},
forwards =
function(msg)
{
x = msg$body
if(length(x) == 0 || sum(nchar(x)) == 0)
return(NA)
ans = length(grep("^[[:space:]]*>", x))
100 * ans / length(x)
},
isOrigMsg =
function(msg)
{
x = msg$body
if(length(x) == 0) return(NA)
length(grep("^[^[:alpha:]]*original[^[:alpha:]]+message[^[:alpha:]]*$",
tolower(x) ) ) > 0
},
isDear =
function(msg)
{
x = msg$body
if(length(x) == 0) return(NA)
length(grep("^[[:blank:]]*dear +(sir|madam)\\>",
tolower(x))) > 0
},
isWrote =
function(msg)
{
x = msg$body
if(length(x) == 0) return(NA)
length(grep("(wrote|schrieb|ecrit|escribe):", tolower(x) )) > 0
},
avgWordLen =
function(msg)
{
txt = paste(msg$body, collapse = " ")
if(length(txt) == 0 || sum(nchar(txt)) == 0) return(0)
txt = gsub("[^[:alpha:]]", " ", txt)
words = unlist(strsplit(txt, "[[:blank:]]+"))
wordLens = nchar(words)
mean(wordLens[ wordLens > 0 ])
}
,
numDlr =
function(msg)
{
x = paste(msg$body, collapse = "")
if(length(x) == 0 || sum(nchar(x)) == 0)
return(NA)
nchar(gsub("[^$]","", x))
}
)
#--------------
# Wordlist that has a very high probability of being spam message if
# the message includes these words
# NOTE: Only used in Subject of message, not body
SpamCheckWords =
c("viagra", "pounds", "free", "weight", "guarantee", "million",
"dollars", "credit", "risk", "prescription", "generic", "drug",
"financial", "save", "dollar", "erotic", "million", "barrister",
"beneficiary", "easy",
"money back", "money", "credit card")
#----------- getMessageRecipients () function - Rev A ----------
# Creates a function that create a list of message recipients in
# the TO:  CC:  and BCC: Fields
# Used in a few functions with funcList list
getMessageRecipients =
function(header)
{
c(if("To" %in% names(header))  header[["To"]] else character(0),
if("Cc" %in% names(header))  header[["Cc"]] else character(0),
if("Bcc" %in% names(header)) header[["Bcc"]] else character(0)
)
}
#-------------
#####
##### Creating derived datafram from messages
##### This dataframe is the basis of our analysis.  All analysis will be
##### performed on these 30 variables
emailDF = createDerivedDF(emailStruct)
dim(emailDF)
#[1] 9348   30 (9348 observations & 30 variables)
print(head(emailDF), row.names = FALSE)
dim(emailDF)
### Confirming that the dataset is actually correct.  To
### do this, we want to use a different method to find one of the
### previously processed variables to see if they are identical
### For this exercise, we chose the %capital statistic
# Finding the % capital letters in a message
perCaps2 =
function(msg)
{
body = paste(msg$body, collapse = "")
# Return NA if the body of the message is "empty"
if(length(body) == 0 || nchar(body) == 0) return(NA)
# Eliminate non-alpha characters and empty lines
body = gsub("[^[:alpha:]]", "", body)
els = unlist(strsplit(body, ""))
ctCap = sum(els %in% LETTERS)
100 * ctCap / length(els)
}
#---------
# Testing both methods for finding % capitals in Message
# to see if they are identical
pC = sapply(emailStruct, perCaps) # Original
pC2 = sapply(emailStruct, perCaps2) # New Method
identical(pC, pC2) # [1] TRUE
# Looking for unusual values in the data
# looking for NA values
emailDFna_count = colSums(is.na(emailDF))
print(emailDFna_count)
# isSpam          isRe      numLines    bodyCharCt    underscore      subExcCt     subQuesCt
# 0             0             0             0             0            20            20
# numAtt      priority        numRec       perCaps   isInReplyTo     sortedRec       subPunc
# 0             0           282             0             0             0             0
# hour multipartText     hasImages   isPGPsigned       perHTML  subSpamWords     subBlanks
# 0             0             0             0             0             7            20
# noHost        numEnd     isYelling      forwards     isOrigMsg        isDear       isWrote
# 1             0             7             0             0             0             0
# avgWordLen   numDlr
# 0             0
# We see the variable with subject line Exclamation points has 20 NA values
# We retrieve the index numbers of these values
indNA = which(is.na(emailDF$subExcCt))
# We check these values against the index values for the messages without a
# subject line
indNoSubject = which(is.na(emailDF$subBlanks))
#indNoSubject = which(sapply(emailStruct,
#                            function(msg)
#                              !("Subject" %in% names(msg$header))))
# Determine if the NA values for subExcCt is the same as the index
# values of the messages without a subject line
all(indNA == indNoSubject) #[1] TRUE
# The number of lines in an email should not be greater than the number
# of characters in the bocy of the email. (detecting empty messages)
all(emailDF$bodyCharCt > emailDF$numLines) #[1] TRUE
x.at = c(1,10,100,1000,10000,100000)
y.at = c(1, 5, 10, 50, 100, 500, 5000)
nL = 1 + emailDF$numLines
nC = 1 + emailDF$bodyCharCt
plot(nL ~ nC, log = "xy", pch=".", xlim=c(1,100000), axes = FALSE,
xlab = "Number of Characters", ylab = "Number of Lines",main=" Number of Characters vs Lines\n Message Body")
box()
axis(1, at = x.at, labels = formatC(x.at, digits = 0, format="d"))
axis(2, at = y.at, labels = formatC(y.at, digits = 0, format="d"))
abline(a=0, b=1, col="red", lwd = 2)
percent = emailDF$perCaps
isSpamLabs = factor(emailDF$isSpam, labels = c("ham", "spam"))
boxplot(log(1 + percent) ~ isSpamLabs,
ylab = "Percent Capitals (log)",cex=.4,pch=3,
main = "Box-Plot % Capitalization - Message Body")
logPerCapsSpam = log(1 + emailDF$perCaps[ emailDF$isSpam ])
logPerCapsHam = log(1 + emailDF$perCaps[ !emailDF$isSpam ])
qqplot(logPerCapsSpam, logPerCapsHam,
xlab = "Regular Email", ylab = "Spam Email",
main = "Percentage of Capital Letters (log scale)",
pch = 19, cex = 0.3)
colI = c("#4DAF4A80", "#984EA380")
logBodyCharCt = log(1 + emailDF$bodyCharCt)
logPerCaps = log(1 + emailDF$perCaps)
plot(logPerCaps ~ logBodyCharCt, xlab = "Total Characters (log)",
ylab = "Percent Capitals (log)",
col = colI[1 + emailDF$isSpam],
xlim = c(2,12), pch = 19, cex = 0.5)
# looking at the number of attachments for spam vs ham (not very telling)
table(emailDF$numAtt, isSpamLabs)
# isSpamLabs
#     ham spam
# 0  6624 2158
# 1   314  230
# 2    11    6
# 4     0    1
# 5     1    2
# 18    1    0
oldPar = par(mfrow = c(1, 2), mar = c(1,1,1,1))
colM = c("#E41A1C80", "#377EB880")
isRe = factor(emailDF$isRe, labels = c("no Re:", "Re:"))
mosaicplot(table(isSpamLabs, isRe), main = "",
xlab = "", ylab = "", color = colM)
fromNE = factor(emailDF$numEnd, labels = c("No #", "#"))
mosaicplot(table(isSpamLabs, fromNE), color = colM,
main = "", xlab="", ylab = "")
par(oldPar)
#----------------------------------------------------------------------------
#-----------  3.11 Fitting the rpart() Model
#----------------------------------------------------------------------------
library(rpart)
# within rpart() all variables must either be factors or numeric. We must
# convert our logical variables to factors
#----------- setupRpart () function - Rev A ----------
# Creates logical variables from factor variables
# Binds new variables to dataframe and deleted original variables
setupRpart = function(data) {
logicalVars = which(sapply(data, is.logical))
facVars = lapply(data[ , logicalVars],
function(x) {
x = as.factor(x)
levels(x) = c("F", "T")
x
})
cbind(facVars, data[ , - logicalVars])
}
# Preparing dataset for rpart() Converting to factor variables
emailDFrp = setupRpart(emailDF)
dim(emailDFrp)
set.seed(418910)
# Split the data set into 2/3 training and 1/3 test sets. This
# is the same split we used for the Naive Bayes dataset, since
# we are using the same seed value.
# First step is to select the Index values that will be used for
# each set of data
testSpamIdx = sample(numSpam, size = floor(numSpam/3))
testHamIdx = sample(numHam, size = floor(numHam/3))
# Creating testDF using the testIdx values. Adding observations
# that are isSpam = T first than isSpam = F after
testDF =
rbind( emailDFrp[ emailDFrp$isSpam == "T", ][testSpamIdx, ],
emailDFrp[emailDFrp$isSpam == "F", ][testHamIdx, ] )
dim(testDF)#[1] 3116   30
# Repeat the process for testDF but using all observations that
# were not included in the testDF
trainDF =
rbind( emailDFrp[emailDFrp$isSpam == "T", ][-testSpamIdx, ],
emailDFrp[emailDFrp$isSpam == "F", ][-testHamIdx, ])
dim(trainDF)#[1] 6232   30
#-------------- Fitting rpart() nmodel to training dataset --
rpartFit = rpart(isSpam ~ ., data = trainDF, method = "class")
# Library for plotting rpart object  (Trees)
library(rpart.plot)
prp(rpartFit, extra = 1) # Function for plotting rpart tree
# Use predict function on the testDF, using rpartFit model just created
# to determine the predicted classification of the testDF messages
predictions = predict(rpartFit,
newdata = testDF[, names(testDF) != "isSpam"],
type = "class")
predictions[1:5]
summary(predictions)
# F    T
# 2317  799
# Finding Type I error for TestDF (predicting T when it is actually F)
# subsetting using the indexes for testDF$isSpam which were classified as F
predsForHam = predictions[ testDF$isSpam == "F" ]
summary(predsForHam)
# F    T
# 2192  125
# The error rate for Type I error
sum(predsForHam == "T") / length(predsForHam)
# [1] 0.05394907
# Repeat for Type II error for TestDF (predicting F when it is actually T)
# subsetting using the indexes for testDF$isSpam which were classified as T
predsForSpam = predictions[ testDF$isSpam == "T" ]
summary(predsForSpam)
# F   T
# 125 674
sum(predsForSpam == "F") / length(predsForSpam)
# [1] 0.1564456
# Creating a list of complexity Values to be used in rpart() function,
# to see if it improves performance
complexityVals = c(seq(0.00001, 0.0001, length=19),
seq(0.0001, 0.001, length=19),
seq(0.001, 0.005, length=9),
seq(0.005, 0.01, length=9))
# Use lapply to create a list of rpart objects, using trainDF, and using
# the different complexity values
fits = lapply(complexityVals, function(x) {
rpartObj = rpart(isSpam ~ ., data = trainDF,
method="class",
control = rpart.control(cp=x) )
# using predict() on the testDF, using the rpart objects
predict(rpartObj,
newdata = testDF[ , names(testDF) != "isSpam"],
type = "class")
})
spam = testDF$isSpam == "T"
numSpam = sum(spam)
numHam = sum(!spam)
# We assign Type I and Type II error rates on the rpartObj
errs = sapply(fits, function(preds) {
typeI = sum(preds[ !spam ] == "T") / numHam
typeII = sum(preds[ spam ] == "F") / numSpam
c(typeI = typeI, typeII = typeII)
})
dim(errs) #[1]  2 56
head(errs[,1:10])
#              [,1]       [,2]       [,3]       [,4]       [,5]       [,6]       [,7]
# typeI  0.04272767 0.04272767 0.04272767 0.04272767 0.04272767 0.04272767 0.04272767
# typeII 0.11639549 0.11639549 0.11639549 0.11639549 0.11639549 0.11639549 0.11639549
# [,8]       [,9]      [,10]
# typeI  0.04272767 0.04272767 0.04272767
# typeII 0.11639549 0.11639549 0.11639549
library(RColorBrewer)
cols = brewer.pal(9, "Set1")[c(3, 4, 5)]
plot(errs[1,] ~ complexityVals, type="l", col=cols[2],
lwd = 2, ylim = c(0,0.2), xlim = c(0,0.005),
ylab="Error", xlab="complexity parameter values")
points(errs[2,] ~ complexityVals, type="l", col=cols[1], lwd = 2)
text(x =c(0.003, 0.0035), y = c(0.12, 0.05),
labels=c("Type II Error", "Type I Error"))
minI = which(errs[1,] == min(errs[1,]))[1]
abline(v = complexityVals[minI], col ="grey", lty =3, lwd=2)
text(0.0007, errs[1, minI]+0.01,
formatC(errs[1, minI], digits = 2))
text(0.0007, errs[2, minI]+0.01,
formatC(errs[2, minI], digits = 3))
save(emailDFrp,file="data.Rda")
library(caret)
install.packages("caret")
library(caret)
setupRnum = function(data) {
logicalVars = which(sapply(data, is.logical))
facVars = lapply(data[ , logicalVars],
function(x) {
x = as.numeric(x)
})
cbind(facVars, data[ , - logicalVars])
}
#----
emailDFnum = setupRnum(emailDF)
emailDFnum[is.na(emailDFnum)]<-0
cv_folds <- createFolds(emailDFnum$isSpam, k=5, list=TRUE, returnTrain = TRUE)
lengths(cv_folds)
?createFolds()
??createFolds()
library(caret)
library(MLmetrics)
install.packages("MLmetrics")
library(MLmetrics)
f1 <- function(data, lev = NULL, model = NULL) {
f1_val <- F1_Score(y_pred = data$pred, y_true = data$obs, positive = lev[1])
p <- Precision(y_pred = data$pred, y_true = data$obs, positive = lev[1])
r <- Recall(y_pred = data$pred, y_true = data$obs, positive = lev[1])
fp <-sum(data$pred==0 & data$obs==1)/length(data$pred)
fn <-sum(data$pred==1 & data$obs==0)/length(data$pred)
c(F1 = f1_val,
prec = p,
rec = r,
Type_I_err=fp,
Type_II_err=fn
)
}
library(naivebayes)
install.packages("naivebayes")
library(e1071)
nb_grid<-expand.grid(laplace=c(0,0.1,0.3,0.5,1), usekernel=c(T,F), adjust=c(T,F))
train_control<-trainControl(method="cv", number=3, savePredictions = 'final',summaryFunction = f1)
model_nb<-caret::train(as.factor(isSpam) ~ .,data=emailDFnum, trControl = train_control, method='naive_bayes',tuneGrid = nb_grid)
model_nb
nb_grid<-expand.grid(laplace=c(0,0.1,0.3,0.5,1), usekernel=c(T,F), adjust=c(T,F))
??trainControl()
install.packages("iterators")
library(caret)
setupRnum = function(data) {
logicalVars = which(sapply(data, is.logical))
facVars = lapply(data[ , logicalVars],
function(x) {
x = as.numeric(x)
})
cbind(facVars, data[ , - logicalVars])
}
#----
emailDFnum = setupRnum(emailDF)
# Imputing all NA values to 0
emailDFnum[is.na(emailDFnum)]<-0
cv_folds <- createFolds(emailDFnum$isSpam, k=5, list=TRUE, returnTrain = TRUE)
lengths(cv_folds)
install.packages("caret")
library(caret)
install.packages("ipred")
install.packages("caret")
library(caret)
install.packages("lava")
install.packages("caret")
library(caret)
setupRnum = function(data) {
logicalVars = which(sapply(data, is.logical))
facVars = lapply(data[ , logicalVars],
function(x) {
x = as.numeric(x)
})
cbind(facVars, data[ , - logicalVars])
}
#----
emailDFnum = setupRnum(emailDF)
# Imputing all NA values to 0
emailDFnum[is.na(emailDFnum)]<-0
cv_folds <- createFolds(emailDFnum$isSpam, k=5, list=TRUE, returnTrain = TRUE)
lengths(cv_folds)
## Finally Metric Stuff
# Because our authors prefer Type I/II errors, but the cool kids
# know that precision/recall/F1 is where its at, while the default of caret
# is accuracy and kappa.  To get us all on the same page, I create a function
# that returns the metrics we want.  However, rather than re-invent the wheel,
# I just install a package.  I am not sure if it had Type I/II errors so those
# I made my self.  \#MLSwag
library(MLmetrics)
f1 <- function(data, lev = NULL, model = NULL) {
f1_val <- F1_Score(y_pred = data$pred, y_true = data$obs, positive = lev[1])
p <- Precision(y_pred = data$pred, y_true = data$obs, positive = lev[1])
r <- Recall(y_pred = data$pred, y_true = data$obs, positive = lev[1])
fp <-sum(data$pred==0 & data$obs==1)/length(data$pred)
fn <-sum(data$pred==1 & data$obs==0)/length(data$pred)
c(F1 = f1_val,
prec = p,
rec = r,
Type_I_err=fp,
Type_II_err=fn
)
}
#----
# ok so lets get the naive bayes packages installed. (first 2 lines)
# The next line makes a dataframe of all the parameters to check.
# If you don't know what they are, look them up
# https://topepo.github.io/caret/available-models.html
#
# Then we create a trainControl object.  It tells caret how to train--using a
# cross-validation ('cv') with 3 folds in this case (number = 3).  We want the
# final predictions of the best model and our summary is the custom function from
# above.
#
# Then we create our model: "model_nb".  We user the caret::train method.  We make
# 'isSpam' a factor because R is dumb and can't figure out that 1 and 0 are classes.
# *as.factor(isSpam) ~ .*  means Y=as.factor(isSpam), X=everything else.
#
# *method* is the package we are using, and we pass our tuning grid.
library(naivebayes)
library(e1071)
nb_grid<-expand.grid(laplace=c(0,0.1,0.3,0.5,1), usekernel=c(T,F), adjust=c(T,F))
train_control<-trainControl(method="cv", number=3, savePredictions = 'final',summaryFunction = f1)
model_nb<-caret::train(as.factor(isSpam) ~ .,data=emailDFnum, trControl = train_control, method='naive_bayes',tuneGrid = nb_grid)
model_nb
table(model_nb$pred['Resample'])
val<-seq(from = 0, to=0.01, by=0.0005)
library(rpart)
cart_grid<-expand.grid(cp=val)
train_control<-trainControl(method="cv", number =5, savePredictions = 'final',summaryFunction = f1)
model_rpart<-caret::train(as.factor(isSpam) ~ .,data=emailDFnum, trControl = train_control, method='rpart',tuneGrid = cart_grid)
model_rpart
install.packages("randomForest")
library(randomForest)
rf_grid<-expand.grid(mtry=seq(from =1, to = 25, by = 2))
train_control<-trainControl(method="cv", number=3, savePredictions = 'final',summaryFunction = f1)
model_rf<-caret::train(as.factor(isSpam) ~ .,data=emailDFnum, trControl = train_control, ntree=200,method='rf',tuneGrid = rf_grid)
model_rpart
install.packages("xgboost")
library(xgboost)
xgb_grid<-expand.grid(nrounds = 100, max_depth = c(3,5,7,9,11), eta = c(0.01,0.03,0.1), gamma=c(1,3,5,10), colsample_bytree=1, min_child_weight=1, subsample=1)
train_control<-trainControl(method="cv", number=3, savePredictions = 'final',summaryFunction = f1)
model_xgb<-caret::train(as.factor(isSpam) ~ .,data=emailDFnum, trControl = train_control,method='xgbTree',tuneGrid = xgb_grid)
model_xgb
library(reprtree)
save(emailDFrp,file="./Data/data.Rda")
