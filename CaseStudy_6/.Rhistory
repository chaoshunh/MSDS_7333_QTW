c(typeI = typeI, typeII = typeII)
})
dim(errs) #[1]  2 56
head(errs[,1:10])
#              [,1]       [,2]       [,3]       [,4]       [,5]       [,6]       [,7]
# typeI  0.04272767 0.04272767 0.04272767 0.04272767 0.04272767 0.04272767 0.04272767
# typeII 0.11639549 0.11639549 0.11639549 0.11639549 0.11639549 0.11639549 0.11639549
# [,8]       [,9]      [,10]
# typeI  0.04272767 0.04272767 0.04272767
# typeII 0.11639549 0.11639549 0.11639549
#------- Figure 3.9 - Type I and II Errors for Recussive Partitioning
library(RColorBrewer)
cols = brewer.pal(9, "Set1")[c(3, 4, 5)]
plot(errs[1,] ~ complexityVals, type="l", col=cols[2],
lwd = 2, ylim = c(0,0.2), xlim = c(0,0.005),
ylab="Error", xlab="complexity parameter values")
points(errs[2,] ~ complexityVals, type="l", col=cols[1], lwd = 2)
text(x =c(0.003, 0.0035), y = c(0.12, 0.05),
labels=c("Type II Error", "Type I Error"))
minI = which(errs[1,] == min(errs[1,]))[1]
abline(v = complexityVals[minI], col ="grey", lty =3, lwd=2)
text(0.0007, errs[1, minI]+0.01,
formatC(errs[1, minI], digits = 2))
text(0.0007, errs[2, minI]+0.01,
formatC(errs[2, minI], digits = 3))
library(caret)
# Our data is in T/F 'factors'.
# We need to change it to numbers.  And as it turns out, there are quite
# a few NANs as well.  Let's set those to zero
setupRnum = function(data) {
logicalVars = which(sapply(data, is.logical))
facVars = lapply(data[ , logicalVars],
function(x) {
x = as.numeric(x)
})
cbind(facVars, data[ , - logicalVars])
}
#----
emailDFnum = setupRnum(emailDF)
# Imputing all NA values to 0
emailDFnum[is.na(emailDFnum)]<-0
## I think i may skip this and see if I can get the folds to work
## internally to caret
#  However, one way to define your folds is to set a seed, and have
#  your folds in a list that you can pass on to others to get the same splits.
cv_folds <- createFolds(emailDFnum$isSpam, k=5, list=TRUE, returnTrain = TRUE)
lengths(cv_folds)
## Finally Metric Stuff
# Because our authors prefer Type I/II errors, but the cool kids
# know that precision/recall/F1 is where its at, while the default of caret
# is accuracy and kappa.  To get us all on the same page, I create a function
# that returns the metrics we want.  However, rather than re-invent the wheel,
# I just install a package.  I am not sure if it had Type I/II errors so those
# I made my self.  \#MLSwag
library(MLmetrics)
f1 <- function(data, lev = NULL, model = NULL) {
f1_val <- F1_Score(y_pred = data$pred, y_true = data$obs, positive = lev[1])
p <- Precision(y_pred = data$pred, y_true = data$obs, positive = lev[1])
r <- Recall(y_pred = data$pred, y_true = data$obs, positive = lev[1])
fp <-sum(data$pred==0 & data$obs==1)/length(data$pred)
fn <-sum(data$pred==1 & data$obs==0)/length(data$pred)
c(F1 = f1_val,
prec = p,
rec = r,
Type_I_err=fp,
Type_II_err=fn
)
}
#----
# ok so lets get the naive bayes packages installed. (first 2 lines)
# The next line makes a dataframe of all the parameters to check.
# If you don't know what they are, look them up
# https://topepo.github.io/caret/available-models.html
#
# Then we create a trainControl object.  It tells caret how to train--using a
# cross-validation ('cv') with 3 folds in this case (number = 3).  We want the
# final predictions of the best model and our summary is the custom function from
# above.
#
# Then we create our model: "model_nb".  We user the caret::train method.  We make
# 'isSpam' a factor because R is dumb and can't figure out that 1 and 0 are classes.
# *as.factor(isSpam) ~ .*  means Y=as.factor(isSpam), X=everything else.
#
# *method* is the package we are using, and we pass our tuning grid.
library(naivebayes)
library(e1071)
nb_grid<-expand.grid(laplace=c(0,0.1,0.3,0.5,1), usekernel=c(T,F), adjust=c(T,F))
train_control<-trainControl(method="cv", number=3, savePredictions = 'final',summaryFunction = f1)
model_nb<-caret::train(as.factor(isSpam) ~ .,data=emailDFnum, trControl = train_control, method='naive_bayes',tuneGrid = nb_grid)
model_nb
#Did the boss fool us with the folds?  Nope.
table(model_nb$pred['Resample'])
# Fold1 Fold2 Fold3
# 3116  3116  3116
val<-seq(from = 0, to=0.01, by=0.0005)
library(rpart)
cart_grid<-expand.grid(cp=val)
train_control<-trainControl(method="cv", number =5, savePredictions = 'final',summaryFunction = f1)
model_rpart<-caret::train(as.factor(isSpam) ~ .,data=emailDFnum, trControl = train_control, method='rpart',tuneGrid = cart_grid)
model_rpart
# CART
#
# 9348 samples
# 29 predictor
# 2 classes: '0', '1'
#
# No pre-processing
# Resampling: Cross-Validated (5 fold)
# Summary of sample sizes: 7478, 7478, 7478, 7479, 7479
# Resampling results across tuning parameters:
#
#   cp      F1         prec       rec        Type_I_err  Type_II_err
# 0.0000  0.9581525  0.9561686  0.9601489  0.03273425  0.02963208
# 0.0005  0.9590785  0.9557457  0.9624508  0.03316246  0.02792033
# 0.0010  0.9612105  0.9581377  0.9643216  0.03134382  0.02652927
# 0.0015  0.9593084  0.9572004  0.9614442  0.03198559  0.02866888
# 0.0020  0.9581564  0.9592122  0.9571287  0.03027430  0.03187755
# 0.0025  0.9575135  0.9553423  0.9597183  0.03337671  0.02995230
# 0.0030  0.9570396  0.9544202  0.9597189  0.03412566  0.02995225
# 0.0035  0.9556337  0.9538952  0.9574167  0.03444657  0.03166411
# 0.0040  0.9546036  0.9548451  0.9543958  0.03359096  0.03391050
# 0.0045  0.9534228  0.9542049  0.9526695  0.03401865  0.03519398
# 0.0050  0.9518631  0.9533941  0.9503674  0.03455284  0.03690624
# 0.0055  0.9500561  0.9530936  0.9470580  0.03466019  0.03936641
# 0.0060  0.9472297  0.9533036  0.9413041  0.03434008  0.04364466
# 0.0065  0.9453650  0.9486385  0.9421674  0.03797650  0.04300295
# 0.0070  0.9437633  0.9432875  0.9443253  0.04225572  0.04139799
# 0.0075  0.9436108  0.9432716  0.9440377  0.04225572  0.04161195
# 0.0080  0.9436108  0.9432716  0.9440377  0.04225572  0.04161195
# 0.0085  0.9436108  0.9432716  0.9440377  0.04225572  0.04161195
# 0.0090  0.9436108  0.9432716  0.9440377  0.04225572  0.04161195
# 0.0095  0.9438597  0.9406092  0.9472031  0.04450205  0.03925878
# 0.0100  0.9423884  0.9385597  0.9463394  0.04610684  0.03990049
#
# F1 was used to select the optimal model using the largest value.
# The final value used for the model was cp = 0.001.
library(randomForest)
rf_grid<-expand.grid(mtry=seq(from =1, to = 25, by = 2))
train_control<-trainControl(method="cv", number=3, savePredictions = 'final',summaryFunction = f1)
model_rf<-caret::train(as.factor(isSpam) ~ .,data=emailDFnum, trControl = train_control, ntree=200,method='rf',tuneGrid = rf_grid)
model_rpart
# CART
#
# 9348 samples
# 29 predictor
# 2 classes: '0', '1'
#
# No pre-processing
# Resampling: Cross-Validated (5 fold)
# Summary of sample sizes: 7478, 7478, 7478, 7479, 7479
# Resampling results across tuning parameters:
#
#   cp      F1         prec       rec        Type_I_err  Type_II_err
# 0.0000  0.9581525  0.9561686  0.9601489  0.03273425  0.02963208
# 0.0005  0.9590785  0.9557457  0.9624508  0.03316246  0.02792033
# 0.0010  0.9612105  0.9581377  0.9643216  0.03134382  0.02652927
# 0.0015  0.9593084  0.9572004  0.9614442  0.03198559  0.02866888
# 0.0020  0.9581564  0.9592122  0.9571287  0.03027430  0.03187755
# 0.0025  0.9575135  0.9553423  0.9597183  0.03337671  0.02995230
# 0.0030  0.9570396  0.9544202  0.9597189  0.03412566  0.02995225
# 0.0035  0.9556337  0.9538952  0.9574167  0.03444657  0.03166411
# 0.0040  0.9546036  0.9548451  0.9543958  0.03359096  0.03391050
# 0.0045  0.9534228  0.9542049  0.9526695  0.03401865  0.03519398
# 0.0050  0.9518631  0.9533941  0.9503674  0.03455284  0.03690624
# 0.0055  0.9500561  0.9530936  0.9470580  0.03466019  0.03936641
# 0.0060  0.9472297  0.9533036  0.9413041  0.03434008  0.04364466
# 0.0065  0.9453650  0.9486385  0.9421674  0.03797650  0.04300295
# 0.0070  0.9437633  0.9432875  0.9443253  0.04225572  0.04139799
# 0.0075  0.9436108  0.9432716  0.9440377  0.04225572  0.04161195
# 0.0080  0.9436108  0.9432716  0.9440377  0.04225572  0.04161195
# 0.0085  0.9436108  0.9432716  0.9440377  0.04225572  0.04161195
# 0.0090  0.9436108  0.9432716  0.9440377  0.04225572  0.04161195
# 0.0095  0.9438597  0.9406092  0.9472031  0.04450205  0.03925878
# 0.0100  0.9423884  0.9385597  0.9463394  0.04610684  0.03990049
#
# F1 was used to select the optimal model using the largest value.
# The final value used for the model was cp = 0.001.
rf1 <- randomForest(isSpam~,cp=0.001,data=emailDFnum)
rf1 <- randomForest(as.factor(isSpam) ~ .,cp=0.001,data=emailDFnum)
rf1$importance
varImpPlot(rf1,main="Important variables for Yield 2015")
rf1 <- randomForest(as.factor(isSpam) ~ .,cp=0.001,data=emailDFnum)
rf1$importance
varImpPlot(rf1,main="Important variables for Spam Classification")
xgb1 <- randomForest(as.factor(isSpam) ~ .,nrounds = 100, max_depth = 11, eta =0.1, gamma = 1,
colsample_bytree = 1, min_child_weight = 1,subsample = 1,data=emailDFnum)
xgb1 <- xgboost(as.factor(isSpam) ~ .,nrounds = 100, max_depth = 11, eta =0.1, gamma = 1,
colsample_bytree = 1, min_child_weight = 1,subsample = 1,data=emailDFnum)
library(xgboost)
xgb_grid<-expand.grid(nrounds = 100, max_depth = c(3,5,7,9,11), eta = c(0.01,0.03,0.1), gamma=c(1,3,5,10), colsample_bytree=1, min_child_weight=1, subsample=1)
train_control<-trainControl(method="cv", number=3, savePredictions = 'final',summaryFunction = f1)
model_xgb<-caret::train(as.factor(isSpam) ~ .,data=emailDFnum, trControl = train_control,method='xgbTree',tuneGrid = xgb_grid)
model_xgb
# eXtreme Gradient Boosting
#
# 9348 samples
# 29 predictor
# 2 classes: '0', '1'
#
# No pre-processing
# Resampling: Cross-Validated (3 fold)
# Summary of sample sizes: 6232, 6232, 6232
# Resampling results across tuning parameters:
#
#   eta   max_depth  gamma  F1         prec       rec        Type_I_err  Type_II_err
# 0.01   3          1     0.9325138  0.8910818  0.9779888  0.08889602  0.016367137
# 0.01   3          3     0.9320846  0.8908973  0.9772695  0.08900300  0.016902011
# 0.01   3          5     0.9321577  0.8909110  0.9774133  0.08900300  0.016795036
# 0.01   3         10     0.9319668  0.8905632  0.9774133  0.08932392  0.016795036
# 0.01   5          1     0.9507558  0.9253922  0.9775572  0.05862217  0.016688062
# 0.01   5          3     0.9509613  0.9256514  0.9777011  0.05840822  0.016581087
# 0.01   5          5     0.9512952  0.9262849  0.9777011  0.05787334  0.016581087
# 0.01   5         10     0.9511802  0.9269723  0.9766940  0.05723149  0.017329910
# 0.01   7          1     0.9591569  0.9425066  0.9764063  0.04428755  0.017543860
# 0.01   7          3     0.9592800  0.9430139  0.9761185  0.04385965  0.017757809
# 0.01   7          5     0.9592591  0.9419007  0.9772695  0.04482242  0.016902011
# 0.01   7         10     0.9586442  0.9424558  0.9753992  0.04428755  0.018292683
# 0.01   9          1     0.9646711  0.9534898  0.9761185  0.03540864  0.017757809
# 0.01   9          3     0.9638535  0.9535405  0.9743922  0.03530167  0.019041506
# 0.01   9          5     0.9634780  0.9536335  0.9735290  0.03519469  0.019683355
# 0.01   9         10     0.9606418  0.9522226  0.9692131  0.03615747  0.022892597
# 0.01  11          1     0.9699404  0.9606401  0.9794274  0.02984596  0.015297390
# 0.01  11          3     0.9692472  0.9614971  0.9771256  0.02909713  0.017008986
# 0.01  11          5     0.9677136  0.9609908  0.9745360  0.02941806  0.018934531
# 0.01  11         10     0.9623347  0.9597878  0.9648971  0.03005991  0.026101840
# 0.03   3          1     0.9425920  0.9119339  0.9753992  0.07006846  0.018292683
# 0.03   3          3     0.9427004  0.9125171  0.9749676  0.06953359  0.018613607
# 0.03   3          5     0.9428897  0.9129983  0.9748238  0.06910569  0.018720582
# 0.03   3         10     0.9422342  0.9117723  0.9748238  0.07017544  0.018720582
# 0.03   5          1     0.9610995  0.9437162  0.9791397  0.04343175  0.015511339
# 0.03   5          3     0.9606605  0.9436751  0.9782765  0.04343175  0.016153188
# 0.03   5          5     0.9605208  0.9435403  0.9781326  0.04353872  0.016260163
# 0.03   5         10     0.9601336  0.9438599  0.9769817  0.04321780  0.017115961
# 0.03   7          1     0.9666731  0.9571373  0.9764063  0.03252033  0.017543860
# 0.03   7          3     0.9661821  0.9564483  0.9761185  0.03305520  0.017757809
# 0.03   7          5     0.9656163  0.9557546  0.9756870  0.03359007  0.018078733
# 0.03   7         10     0.9634656  0.9522349  0.9749676  0.03637142  0.018613607
# 0.03   9          1     0.9712868  0.9621704  0.9805783  0.02866923  0.014441592
# 0.03   9          3     0.9706047  0.9627737  0.9785642  0.02813436  0.015939238
# 0.03   9          5     0.9697869  0.9607502  0.9789958  0.02973898  0.015618314
# 0.03   9         10     0.9663878  0.9587931  0.9741044  0.03112965  0.019255456
# 0.03  11          1     0.9745092  0.9673970  0.9817292  0.02460419  0.013585794
# 0.03  11          3     0.9728531  0.9662287  0.9795713  0.02545999  0.015190415
# 0.03  11          5     0.9720513  0.9659118  0.9782765  0.02567394  0.016153188
# 0.03  11         10     0.9681005  0.9625965  0.9736729  0.02813436  0.019576380
# 0.10   3          1     0.9654720  0.9481370  0.9834556  0.04000856  0.012302097
# 0.10   3          3     0.9645000  0.9468022  0.9828802  0.04107831  0.012729996
# 0.10   3          5     0.9630905  0.9452830  0.9815854  0.04225503  0.013692769
# 0.10   3         10     0.9622365  0.9429776  0.9823047  0.04418057  0.013157895
# 0.10   5          1     0.9741426  0.9648704  0.9835995  0.02663671  0.012195122
# 0.10   5          3     0.9739233  0.9648531  0.9831679  0.02663671  0.012516046
# 0.10   5          5     0.9711921  0.9606069  0.9820170  0.02995293  0.013371844
# 0.10   5         10     0.9651984  0.9513809  0.9794274  0.03722721  0.015297390
# 0.10   7          1     0.9774265  0.9707711  0.9841749  0.02203680  0.011767223
# 0.10   7          3     0.9750744  0.9682310  0.9820170  0.02396234  0.013371844
# 0.10   7          5     0.9721643  0.9647299  0.9797151  0.02663671  0.015083440
# 0.10   7         10     0.9683722  0.9590815  0.9778449  0.03102268  0.016474112
# 0.10   9          1     0.9796257  0.9737140  0.9856136  0.01979033  0.010697475
# 0.10   9          3     0.9753317  0.9695802  0.9811538  0.02289260  0.014013693
# 0.10   9          5     0.9736348  0.9672104  0.9801467  0.02471117  0.014762516
# 0.10   9         10     0.9694661  0.9616518  0.9774133  0.02899016  0.016795036
# 0.10  11          1     0.9810470  0.9755380  0.9866206  0.01839966  0.009948652
# 0.10  11          3     0.9768072  0.9720766  0.9815854  0.02096705  0.013692769
# 0.10  11          5     0.9746716  0.9695395  0.9798590  0.02289260  0.014976466
# 0.10  11         10     0.9712818  0.9646684  0.9779888  0.02663671  0.016367137
#
# Tuning parameter 'nrounds' was held constant at a value of 100
# Tuning
# parameter 'min_child_weight' was held constant at a value of 1
# Tuning
# parameter 'subsample' was held constant at a value of 1
# F1 was used to select the optimal model using the largest value.
# The final values used for the model were nrounds = 100, max_depth = 11, eta =
#   0.1, gamma = 1, colsample_bytree = 1, min_child_weight = 1 and subsample = 1.
xgb1 <- xgboost(as.factor(isSpam) ~ .,nrounds = 100, max_depth = 11, eta =0.1, gamma = 1,
colsample_bytree = 1, min_child_weight = 1,subsample = 1,data=emailDFnum)
xgb1 <- xgboost(as.factor(isSpam) ~ .,nrounds = 100, max_depth = 11, eta =0.1, gamma = 1,
colsample_bytree = 1, min_child_weight = 1,subsample = 1,data=as.matrix(emailDFnum))
xgbdata=as.matrix(emailDFnum)
View(xgbdata)
xgb1 <- xgboost(isSpam ~ .,nrounds = 100, max_depth = 11, eta =0.1, gamma = 1,
colsample_bytree = 1, min_child_weight = 1,subsample = 1,data=as.matrix(emailDFnum))
xgb1 <- xgboost(isSpam ~ .,nrounds = 100, max_depth = 11, eta =0.1, gamma = 1,
colsample_bytree = 1, min_child_weight = 1,subsample = 1,data=xgbdata)
dtrain <- xgb.DMatrix(data = emailDFnum$data, label = emailDFnum$label)
dtrain <- xgb.DMatrix(data = emailDFnum, label = emailDFnum$label)
source('~/GitRepositories/MSDS_7333_QTW/CaseStudy_6/CaseStudy6_Code_RevC.R', echo=TRUE)
dtrain <- xgb.DMatrix(data = emailDFnum, label = emailDFnum)
dev.off()
#------- Figure 3.Special - Variable Importance Factor for Random Forest
pdf("./Figures/Fig3.Special_VariableImportance_RandomForest.pdf", width = 8, height = 7)
rf1 <- randomForest(as.factor(isSpam) ~ .,cp=0.001,data=emailDFnum)
rf1$importance
varImpPlot(rf1,main="Important variables for Spam Classification")
dev.off()
#-----
data(agaricus.train, package='xgboost')
View(agaricus.train)
bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label, max_depth = 2,
eta = 1, nthread = 2, nrounds = 2,objective = "binary:logistic")
xgb.importance(colnames(agaricus.train$data), model = bst)
xgb.importance(colnames(agaricus.train$data), model = bst,
data = agaricus.train$data, label = agaricus.train$label)
xgb.importance(model = bst)
nclass <- 3
nrounds <- 10
mbst <- xgboost(data = as.matrix(iris[, -5]), label = as.numeric(iris$Species) - 1,
max_depth = 3, eta = 0.2, nthread = 2, nrounds = nrounds,
objective = "multi:softprob", num_class = nclass)
# all classes clumped together:
xgb.importance(model = mbst)
# inspect importances separately for each class:
xgb.importance(model = mbst, trees = seq(from=0, by=nclass, length.out=nrounds))
xgb.importance(model = mbst, trees = seq(from=1, by=nclass, length.out=nrounds))
xgb.importance(model = mbst, trees = seq(from=2, by=nclass, length.out=nrounds))
# multiclass classification using gblinear:
mbst <- xgboost(data = scale(as.matrix(iris[, -5])), label = as.numeric(iris$Species) - 1,
booster = "gblinear", eta = 0.2, nthread = 1, nrounds = 15,
objective = "multi:softprob", num_class = nclass)
xgb.importance(model = mbst)
iris
library(ggplot2)
p<-ggplot(testLLR, aes(x=spamLab, fill=dose)) +
geom_boxplot()
p<-ggplot(testLLR ~ spamLab) +
geom_boxplot()
#--- Message Body
pdf("./Figures/Fig3.5_SPAM_boxplotsPercentCaps.pdf", width = 5, height = 5)
percent = emailDF$perCaps
isSpamLabs = factor(emailDF$isSpam, labels = c("ham", "spam"))
boxplot(log(1 + percent) ~ isSpamLabs,
ylab = "Percent Capitals (log)",cex=.4,pch=3,
col = 'lightblue',
main = "Box-Plot % Capitalization - Message Body")
dev.off()
#-------
bst <- xgboost(data = emailDFnum[,-isSpam], label = emailDFnum[,isSpam], max_depth = 2,
eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
bst <- xgboost(data = emailDFnum[,-'isSpam'], label = emailDFnum[,'isSpam'], max_depth = 2,
eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
bst <- xgboost(data = select(emailDFnum, -isSpam), label = emailDFnum$isSpam, max_depth = 2,
eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
bst <- xgboost(data = emailDFnum[ , names(emailDFnum) != "isSpam"], label = emailDFnum$isSpam, max_depth = 2,
eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
bst <- xgboost(data = as.matrix(emailDFnum[ , names(emailDFnum) != "isSpam"]), label = emailDFnum$isSpam, max_depth = 2,
eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
xgb.importance(model = bst)
bst <- xgboost(data = as.matrix(emailDFnum[ , names(emailDFnum) != "isSpam"]), label = emailDFnum$isSpam,
nrounds = 100, max_depth = 11, eta =0.1, gamma = 1, colsample_bytree = 1,
min_child_weight = 1, subsample = 1, objective = "binary:logistic")
xgb.importance(model = bst)
bstimportance = xgb.importance(model = bst)
varImpPlot(bstimportance,main="Important variables for Spam Classification")
importance_matrix <- xgb.importance(colnames(emailDFnum[ , names(emailDFnum) != "isSpam"]), model = bst)
xgb.plot.importance(importance_matrix, rel_to_first = TRUE, xlab = "Relative importance")
(gg <- xgb.ggplot.importance(importance_matrix, measure = "Frequency", rel_to_first = TRUE))
gg + ggplot2::ylab("Frequency")
xgb.plot.importance(importance_matrix, rel_to_first = TRUE, xlab = "Relative importance")
(gg <- xgb.ggplot.importance(importance_matrix, measure = "Frequency", rel_to_first = TRUE))
gg + ggplot2::ylab("Frequency")
xgb.plot.importance(importance_matrix, rel_to_first = TRUE, xlab = "Relative importance")
xgb.plot.importance(importance_matrix, rel_to_first = TRUE,
xlab = "Relative importance",
main = 'Important variables for Spam Classification\nXGBoost',
col='lightblue')
xgb.plot.importance(importance_matrix, rel_to_first = TRUE,
xlab = "Relative importance",
main = 'Important variables for Spam Classification\nXGBoost',
fill='lightblue')
xgb.plot.importance(importance_matrix, rel_to_first = TRUE,
xlab = "Relative importance",
main = 'Important variables for Spam Classification\nXGBoost',
colour='lightblue')
gg <- xgb.ggplot.importance(importance_matrix, measure = "Frequency", rel_to_first = TRUE)
install.packages("Ckmeans.1d.dp")
gg <- xgb.ggplot.importance(importance_matrix, measure = "Frequency", rel_to_first = TRUE)
gg + ggplot2::ylab("Frequency")
xgb.plot.importance(importance_matrix, rel_to_first = TRUE,
xlab = "Relative importance",
main = 'Important variables for Spam Classification\nXGBoost',
col = rainbow(3))
#------- Figure 3.Special - Variable Importance Factor for Random Forest
pdf("./Figures/Fig3.Special2_VariableImportance_XGBoost.pdf", width = 8, height = 7)
# Used optimal results from XGBoost matrix for analysis model
bst <- xgboost(data = as.matrix(emailDFnum[ , names(emailDFnum) != "isSpam"]), label = emailDFnum$isSpam,
nrounds = 100, max_depth = 11, eta =0.1, gamma = 1, colsample_bytree = 1,
min_child_weight = 1, subsample = 1, objective = "binary:logistic")
#bstimportance = xgb.importance(model = bst)
importance_matrix <- xgb.importance(colnames(emailDFnum[ , names(emailDFnum) != "isSpam"]), model = bst)
xgb.plot.importance(importance_matrix, rel_to_first = TRUE,
xlab = "Relative importance",
main = 'Important variables for Spam Classification\nXGBoost',
col = rainbow(3))
dev.off()
#-----
#------- Figure 3.Special - Variable Importance Factor for Random Forest
pdf("./Figures/Fig3.Special2_VariableImportance_XGBoost.pdf", width = 10, height = 7)
# Used optimal results from XGBoost matrix for analysis model
bst <- xgboost(data = as.matrix(emailDFnum[ , names(emailDFnum) != "isSpam"]), label = emailDFnum$isSpam,
nrounds = 100, max_depth = 11, eta =0.1, gamma = 1, colsample_bytree = 1,
min_child_weight = 1, subsample = 1, objective = "binary:logistic")
#bstimportance = xgb.importance(model = bst)
importance_matrix <- xgb.importance(colnames(emailDFnum[ , names(emailDFnum) != "isSpam"]), model = bst)
xgb.plot.importance(importance_matrix, rel_to_first = TRUE,
xlab = "Relative importance",
main = 'Important variables for Spam Classification\nXGBoost',
col = rainbow(3))
dev.off()
#-----
#------- Figure 3.Special - Variable Importance Factor for Random Forest
pdf("./Figures/Fig3.Special2_VariableImportance_XGBoost.pdf", width = 7, height = 7)
# Used optimal results from XGBoost matrix for analysis model
bst <- xgboost(data = as.matrix(emailDFnum[ , names(emailDFnum) != "isSpam"]), label = emailDFnum$isSpam,
nrounds = 100, max_depth = 11, eta =0.1, gamma = 1, colsample_bytree = 1,
min_child_weight = 1, subsample = 1, objective = "binary:logistic")
#bstimportance = xgb.importance(model = bst)
importance_matrix <- xgb.importance(colnames(emailDFnum[ , names(emailDFnum) != "isSpam"]), model = bst)
xgb.plot.importance(importance_matrix, rel_to_first = TRUE,
xlab = "Relative importance",
main = 'Important variables for Spam Classification\nXGBoost',
col = rainbow(3))
dev.off()
#-----
#------- Figure 3.Special - Variable Importance Factor for Random Forest
pdf("./Figures/Fig3.Special2_VariableImportance_XGBoost.pdf", width = 6.5, height = 7)
# Used optimal results from XGBoost matrix for analysis model
bst <- xgboost(data = as.matrix(emailDFnum[ , names(emailDFnum) != "isSpam"]), label = emailDFnum$isSpam,
nrounds = 100, max_depth = 11, eta =0.1, gamma = 1, colsample_bytree = 1,
min_child_weight = 1, subsample = 1, objective = "binary:logistic")
#bstimportance = xgb.importance(model = bst)
importance_matrix <- xgb.importance(colnames(emailDFnum[ , names(emailDFnum) != "isSpam"]), model = bst)
xgb.plot.importance(importance_matrix, rel_to_first = TRUE,
xlab = "Relative importance",
main = 'Important variables for Spam Classification\nXGBoost',
col = rainbow(3))
dev.off()
#-----
#------- Figure 3.Special - Variable Importance Factor for Random Forest
pdf("./Figures/Fig3.Special2_VariableImportance_XGBoost.pdf", width = 7, height = 7)
# Used optimal results from XGBoost matrix for analysis model
bst <- xgboost(data = as.matrix(emailDFnum[ , names(emailDFnum) != "isSpam"]), label = emailDFnum$isSpam,
nrounds = 100, max_depth = 11, eta =0.1, gamma = 1, colsample_bytree = 1,
min_child_weight = 1, subsample = 1, objective = "binary:logistic")
#bstimportance = xgb.importance(model = bst)
importance_matrix <- xgb.importance(colnames(emailDFnum[ , names(emailDFnum) != "isSpam"]), model = bst)
xgb.plot.importance(importance_matrix, rel_to_first = TRUE,
xlab = "Relative importance",
main = 'Important variables for Spam Classification\nXGBoost',
col = rainbow(3))
dev.off()
#-----
dev.off()
#------- Figure 3.Special - Variable Importance Factor for Random Forest
pdf("./Figures/Fig3.Special2_VariableImportance_XGBoost.pdf", width = 7, height = 7)
# Used optimal results from XGBoost matrix for analysis model
bst <- xgboost(data = as.matrix(emailDFnum[ , names(emailDFnum) != "isSpam"]), label = emailDFnum$isSpam,
nrounds = 100, max_depth = 11, eta =0.1, gamma = 1, colsample_bytree = 1,
min_child_weight = 1, subsample = 1, objective = "binary:logistic")
#bstimportance = xgb.importance(model = bst)
importance_matrix <- xgb.importance(colnames(emailDFnum[ , names(emailDFnum) != "isSpam"]), model = bst)
xgb.plot.importance(importance_matrix, rel_to_first = TRUE,
xlab = "Relative importance",
main = 'Important variables for Spam Classification\nXGBoost',
col = rainbow(3))
dev.off()
#-----
gg <- xgb.ggplot.importance(importance_matrix, measure = "Frequency", rel_to_first = TRUE)
gg + ggplot2::ylab("Frequency")
#------- Figure 3.Special - Variable Importance Factor for Random Forest
pdf("./Figures/Fig3.Special2_VariableImportance_XGBoost.pdf", width = 7, height = 7)
# Used optimal results from XGBoost matrix for analysis model
bst <- xgboost(data = as.matrix(emailDFnum[ , names(emailDFnum) != "isSpam"]), label = emailDFnum$isSpam,
nrounds = 100, max_depth = 11, eta =0.1, gamma = 1, colsample_bytree = 1,
min_child_weight = 1, subsample = 1, objective = "binary:logistic")
#bstimportance = xgb.importance(model = bst)
importance_matrix <- xgb.importance(colnames(emailDFnum[ , names(emailDFnum) != "isSpam"]), model = bst)
xgb.plot.importance(importance_matrix, rel_to_first = TRUE, fill='blue',
xlab = "Relative importance",
main = 'Important variables for Spam Classification\nXGBoost',
col = rainbow(3))
dev.off()
#-----
dev.off()
#------- Figure 3.Special - Variable Importance Factor for Random Forest
pdf("./Figures/Fig3.Special2_VariableImportance_XGBoost.pdf", width = 7, height = 7)
# Used optimal results from XGBoost matrix for analysis model
bst <- xgboost(data = as.matrix(emailDFnum[ , names(emailDFnum) != "isSpam"]), label = emailDFnum$isSpam,
nrounds = 100, max_depth = 11, eta =0.1, gamma = 1, colsample_bytree = 1,
min_child_weight = 1, subsample = 1, objective = "binary:logistic")
#bstimportance = xgb.importance(model = bst)
importance_matrix <- xgb.importance(colnames(emailDFnum[ , names(emailDFnum) != "isSpam"]), model = bst)
xgb.plot.importance(importance_matrix, rel_to_first = TRUE, fill='blue',
xlab = "Relative importance",
main = 'Important variables for Spam Classification\nXGBoost',
col = rainbow(3))
dev.off()
#-----
#------- Figure 3.Special - Variable Importance Factor for Random Forest
pdf("./Figures/Fig3.Special2_VariableImportance_XGBoost.pdf", width = 7, height = 7)
# Used optimal results from XGBoost matrix for analysis model
bst <- xgboost(data = as.matrix(emailDFnum[ , names(emailDFnum) != "isSpam"]), label = emailDFnum$isSpam,
nrounds = 100, max_depth = 11, eta =0.1, gamma = 1, colsample_bytree = 1,
min_child_weight = 1, subsample = 1, objective = "binary:logistic")
#bstimportance = xgb.importance(model = bst)
importance_matrix <- xgb.importance(colnames(emailDFnum[ , names(emailDFnum) != "isSpam"]), model = bst)
xgb.plot.importance(importance_matrix, rel_to_first = TRUE,
xlab = "Relative importance",
main = 'Important variables for Spam Classification\nXGBoost',
col = 'blue')
dev.off()
#-----
?xgb.plot.importance()
